# agents.toml — LM-AIG Agent Configuration
# Paper-recommended values are the defaults.
# Only override what you need to change.
#
# If an agent has no "model" key, it falls back to [defaults].model
# If an agent has no "groq_model"/"ollama_model", it falls back to [providers.X].default_model

[defaults]
model = "meta-llama/llama-4-maverick"
timeout = 120                # Request timeout per provider (seconds). On timeout → fallback.
min_response_length = 50     # Minimum chars from LLM. Below this → try next provider.

# ---- Retry Policy (LangGraph node-level retry) ----

[retry]
max_attempts = 3           # Total attempts per node (1 initial + 2 retries)
initial_interval = 1.0     # Seconds before first retry
backoff_factor = 2.0       # Exponential backoff multiplier

# ---- Fallback Providers ----
# Chain: OpenRouter (primary) → Groq (fallback 1) → Ollama (fallback 2)
# Set enabled = false to skip a provider.

[providers.groq]
enabled = true
default_model = "llama-3.3-70b-versatile"

[providers.ollama]
enabled = true
default_model = "gpt-oss:20b"
base_url = "http://localhost:11434"

# ---- Agent Configurations ----
# groq_model / ollama_model: agent-specific fallback model overrides
# If omitted, uses [providers.X].default_model

[agents.websurfer]
model = "meta-llama/llama-4-scout"
temperature = 0.0          # Factual accuracy (paper)
max_results = 5            # Tavily search results to retrieve
search_depth = "advanced"  # Tavily search depth: "basic" or "advanced"
cache_enabled = true       # Cache search results per construct (avoid redundant API calls)
cache_ttl_hours = 24       # Hours before cached results expire
groq_model = "llama-3.3-70b-versatile"
ollama_model = "gpt-oss:20b"

[agents.item_writer]
model = "meta-llama/llama-4-maverick"
temperature = 1.0          # Creative diversity (paper)
num_items = 8              # Items to generate per cycle
groq_model = "llama-3.3-70b-versatile"
ollama_model = "gpt-oss:20b"

[agents.content_reviewer]
model = "meta-llama/llama-4-maverick"
temperature = 0.0          # Deterministic evaluation
groq_model = "llama-3.3-70b-versatile"
ollama_model = "gpt-oss:20b"

[agents.linguistic_reviewer]
model = "meta-llama/llama-4-maverick"
temperature = 0.0          # Deterministic evaluation
groq_model = "llama-3.3-70b-versatile"
ollama_model = "gpt-oss:20b"

[agents.bias_reviewer]
model = "meta-llama/llama-4-maverick"
temperature = 0.0          # Deterministic evaluation
groq_model = "llama-3.3-70b-versatile"
ollama_model = "gpt-oss:20b"

[agents.meta_editor]
model = "meta-llama/llama-4-maverick"
temperature = 0.3          # Slight creativity for synthesis
groq_model = "llama-3.3-70b-versatile"
ollama_model = "gpt-oss:20b"

[agents.lewmod]
model = "meta-llama/llama-4-maverick"
temperature = 0.3          # Balanced expert evaluation
groq_model = "llama-3.3-70b-versatile"
ollama_model = "gpt-oss:20b"

[agents.injection_classifier]
model = "meta-llama/llama-4-scout"     # Fast/lightweight for classification
temperature = 0.0                       # Deterministic verdicts
groq_model = "llama-3.3-70b-versatile"
ollama_model = "gpt-oss:20b"

[workflow]
max_revisions = 5          # Default revision rounds (CLI --max-revisions overrides)
memory_enabled = true      # Use previous run items to push for diversity (anti-homogeneity)
memory_limit = 5           # How many prior runs' items to include in prompts

[json_fix]
max_attempts = 8
memory_window = 4

# ---- API Server Configuration ----
# Used by FastAPI server (src/api/app.py). Ignored by CLI.

# ---- Prompt Injection Defense ----
# Two-layer LLM-based defense for human feedback input.
# Layer 1: Intent classification (jailbreak detection)
# Layer 2: Domain boundary check (psychometric scope)

[prompt_injection]
enabled = true
threshold = 0.7            # Confidence >= this triggers STOP (0.0-1.0)
min_input_length = 10      # Skip check for inputs shorter than this (chars)

# ---- API Server Configuration ----
# Used by FastAPI server (src/api/app.py). Ignored by CLI.

[api]
max_workers = 10               # Global max concurrent pipeline runs
max_concurrent_per_user = 3    # Per-user concurrent run limit
rate_limit_rpm = 10            # Requests per minute per API key
rate_limit_daily = 100         # Requests per day per API key
